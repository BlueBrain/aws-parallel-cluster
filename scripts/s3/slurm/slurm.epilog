#!/bin/bash

#####################################################################
# Cleans-up any temporary resources associated with the job, stores #
# logs and other related information, and more. Additionally, it    #
# kills any user processes on the node when the last SLURM job ends #
# there. For example, if a user logs into an allocated node SLURM,  #
# it will not kill its processes without this epilog script.        #
#####################################################################

# Perform common checks and set required variables (e.g., TMPDIR)
. /opt/slurm/etc/scripts/slurm.common

# INFRA-2142 & HELP-15810 & HPCTM-1633
  # Only run on the 'main' node or 'batch' node
  if [[ ${SLURMD_NODENAME} == $(scontrol_get BatchHost) ]]; then
    # INFRA-2142
      # Store >final< info and CloudWatch URLs
      scontrol_cmd ${SLURM_JOB_ID_EXT} > $(logfile info)  # Extended job ID prevents wrong outputs in job arrays
      cloudwatch_urls "false" $(scontrol_get EndTime) > $(logfile cloudwatch)

      # Copy 'stdout' and link 'stderr' file if identical
      cptrim "$(scontrol_get StdOut)" $(logfile stdout txt ${SHMDIR})
      if [[ -z $(diff "$(scontrol_get StdOut)" "$(scontrol_get StdErr)" 2>&1) ]]; then
        ln -s $(logfile stdout) $(logfile stderr)
      else
        cptrim "$(scontrol_get StdErr)" $(logfile stderr txt ${SHMDIR})
      fi

    # Add a header for CloudWatch and copy the job outputs (e.g., SLURM script, Neurodamus logs, etc.)
    rm -f $(SLURM_STEP_ID=* logfile_shm pydamus mtime)
    for file in $(set -f && logfile * * ${SHMDIR} && set +f); do
        header="$(head -n 1 $(logfile info))"
        step_id=$(echo -n ${file} | sed -r "s|^.*-([0-9]+)\.[a-z]{2,4}$|\1|" | grep -v ${file})
        
        # Append the step ID, if required (e.g., when running more than one simulation in Neurodamus)
        if [[ -n ${step_id} ]]; then
          header="$(echo -n ${header} | sed -r "s|^(.*) (JobName=.*)$|\1 StepId=${step_id} \2|")"
          sleep 1  # Note: Prevents CloudWatch Agent to interpret steps as single log, despite pattern
        fi

        sed -i -e "1 s|^|${header}\n\n|" ${file}
        mv ${file} ${SLURM_LOG_DIR}
    done

    # Assign 'slurm' as owner of all copied files and symbolic links in the log directory
    chown --no-dereference slurm:slurm $(set -f && logfile * * && set +f)
    chmod 644 $(set -f && logfile * * && set +f)
  fi

# HELP-9706
  # Clean-up the temporary path (e.g., on Lustre FSx or NVMe drives)
  rm -rf ${TMPDIR}

# HELP-15578 & HELP-9299
  # Clean-up the SHM directory created for the job
  rm -rf ${SHMDIR}

# Check if the user has more jobs running in the current node
  if [[ $(numjobs_node --user=${SLURM_JOB_UID}) -gt 1 ]]; then
    exit 0
  fi

  # No other SLURM jobs for the user, purge processes and leftovers (except root / systemd)
  if [[ ${SLURM_JOB_UID} -gt 100 ]]; then
    pkill -KILL -U ${SLURM_JOB_UID}
    rm -rf $(find /tmp /dev/shm -xdev -user ${SLURM_JOB_USER} 2>/dev/null)
  fi

# HELP-9551
  # Clean-up the SHM directory created for the user
  rm -rf ${SHMDIR_USER}

# Return 0 to prevent draining nodes after an error
exit 0
