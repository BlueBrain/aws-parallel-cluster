#!/bin/bash

#####################################################################
# Configures the environment for the user before launching its job. #
#####################################################################

# Perform common checks and set essential variables (e.g., TMPDIR)
. /opt/slurm/etc/scripts/slurm.common --essentials

# Check if 'PATH' is not properly set and configure it (i.e., for SLURM REST API jobs)
  if [[ ${PATH} != *"/opt/aws/bin"* ]]; then
    export PATH=$(echo "/opt/intel/mpi/2021.9.0/libfabric/bin:/opt/intel/mpi/2021.9.0/bin:/sbo/home/${SLURM_JOB_USER}/.local/bin:" \
                       "/opt/amazon/efa/bin/:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/bin:" \
                       "/opt/aws/bin:/opt/parallelcluster/pyenv/versions/3.9.17/envs/cfn_bootstrap_virtualenv/bin:" \
                       "/opt/parallelcluster/pyenv/versions/3.9.17/envs/awsbatch_virtualenv/bin:/opt/slurm/bin" | sed "s|: |:|g")
    echo "export PATH=${PATH}"
  fi

# Configure '--cpus-per-task' to 1 by default, if not set
  if [[ -z ${SLURM_CPUS_PER_TASK} ]]; then
    echo "export SLURM_CPUS_PER_TASK=1"
  fi

# Keep the job environment from the user if not stored yet
  if [[ ! -f $(realpath ${SHMDIR}/*-env.sh) ]]; then
    . /opt/slurm/etc/scripts/slurm.common  # Load the rest of the functionality
    if [[ -f ${SLURM_JOB_SCRIPT} ]]; then
      cp ${SLURM_JOB_SCRIPT} $(logfile script sh ${SHMDIR})
    fi
    env > $(logfile env sh ${SHMDIR})
  fi

# Configure behaviour if any task exits with non-zero
  echo "export SLURM_KILL_BAD_EXIT=1"

# Set inactivity timeout to 5 minutes
  echo "export TMOUT=300"

# HELP-9706
  # Export the ENV variable of the temporary directory
  echo "export TMPDIR=${TMPDIR}"

  # Warn users when a node cannot access its own NVMe mount point
  if [[ -b ${NVME_DEV} && -z $(findmnt ${NVME_MNT}) ]]; then
    echo "print [${SLURMD_NODENAME}] WARNING: '${NVME_MNT}' not available. Please, report issue to HPC Team and Core Services."
  fi

# HELP-15578
  # Export the ENV variable of the SHM directory
  echo "export SHMDIR=${SHMDIR}"

# HELP-15810
  # Export the ENV variable to configure Caliper with a suffix per step
  if  [[ -n ${NEURODAMUS_CALI_ENABLED} && "${CALI_MPIREPORT_FILENAME}" == "/dev/null" && -n ${SLURM_NTASKS} ]]; then
    . /opt/slurm/etc/scripts/slurm.common  # Load the rest of the functionality
    echo "export CALI_MPIREPORT_FILENAME=$(logfile_shm perf json)"
  fi

# Bind Lustre FSx alongside several commands within Singularity containers
  SINGULARITY_BIND=${LUSTRE_MNT},$(which lfs),$(which lfs_migrate),$(which ldconfig)
  SINGULARITY_CONTAINLIBS=$(ldconfig -p |& \
                            grep -E "/libnl|/libefa|/libib|/librdma|/libacm|/liblustreapi|/liblnetconfig|/libyaml" | \
                            awk '{print $NF}' | tr '\n' ',' | sed -r "s|,$||")
  echo "export SINGULARITY_BIND=${SINGULARITY_BIND}"
  echo "export SINGULARITY_CONTAINLIBS=${SINGULARITY_CONTAINLIBS}"

# BBPP154-53 + HPCTM-1782
  # If Intel MPI is loaded and not included in the Singularity list, add its libraries
  if [[ -n ${I_MPI_ROOT} && ${SINGULARITY_CONTAINLIBS} != *${I_MPI_ROOT}* ]]; then
    I_MPI_LIBRARIES=$(find ${I_MPI_ROOT}/lib/{,mpi} -maxdepth 1 -type "f,l" | grep -vE "debug|release" | tr '\n' ',')
    EFA_LIBRARIES=$(find /opt/amazon/efa/lib64 -maxdepth 1 -type "f,l" | tr '\n' ',')
    echo "export SINGULARITY_BIND=$(echo -n ${I_MPI_ROOT}/libfabric/lib,${I_MPI_ROOT}/etc,${SINGULARITY_BIND} | sed -r "s|(.*),$|\1|")"
    echo "export SINGULARITY_CONTAINLIBS=$(echo -n ${I_MPI_LIBRARIES}${EFA_LIBRARIES}${SINGULARITY_CONTAINLIBS} | sed -r "s|(.*),$|\1|")"
    echo "export SINGULARITYENV_LD_PRELOAD=/.singularity.d/libs/libmpi.so:/.singularity.d/libs/libfabric.so"
    # Check whether "EFA" is supported in this compute instance and force enable it if it is
    if [[ -n $(fi_info -p efa -t FI_EP_RDM 2>&1 | grep -e "provider: efa") ]]; then
        echo "export I_MPI_OFI_LIBRARY_INTERNAL=0"
        echo "export I_MPI_OFI_PROVIDER=efa"
    fi
  fi

# Return 0 to prevent draining nodes after an error
exit 0
