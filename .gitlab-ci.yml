stages:
 - info
 - start
 - deploy

.aws-cli:
  image: public.ecr.aws/aws-cli/aws-cli:2.11.27
  before_script:
    - mkdir -p ~/.aws/
    - echo -e "[default]\naws_access_key_id = ${AWS_ACCESS_KEY}\naws_secret_access_key = ${AWS_SECRET_ACCESS_KEY}\n" > ~/.aws/credentials
    - echo -e "[default]\nregion = us-east-1\noutput = json\n" > ~/.aws/config

.pcluster:
  image: public.ecr.aws/parallelcluster/pcluster-api:3.6.0
  before_script:
    - |
      # Install the aws cli
      yum install -y unzip jq
      curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
      unzip -q awscliv2.zip
      ./aws/install
    - mkdir -p ~/.aws/
    - echo -e "[default]\naws_access_key_id = ${AWS_ACCESS_KEY}\naws_secret_access_key = ${AWS_SECRET_ACCESS_KEY}\n" > ~/.aws/credentials
    - echo -e "[default]\nregion = us-east-1\noutput = json\n" > ~/.aws/config

.update_or_create_cluster:
  extends: .pcluster
  script:
    - pcluster list-clusters
    - |
      if $(pcluster list-clusters | grep -q hpc-cluster); then
          echo "Cluster exists, will attempt to update."
          echo -n "Checking fleet status: "
          fleet_status=$(pcluster describe-compute-fleet --cluster-name hpc-cluster --query status)
          if [[ "$fleet_status" != *"STOPPED"* ]]; then
              echo "Fleet is not stopped, please stop and rerun validation."
              exit 1
          fi
          echo "PCLUSTER_ACTION=update" > build.env
          set +e
          pcluster update-cluster --cluster-configuration ./config/compute-cluster.yaml --cluster-name hpc-cluster --dryrun $DRYRUN 2>&1 > pcluster-output.log
          pcluster_status=$?
          set -e
      else
          echo "Creating new cluster"
          echo "PCLUSTER_ACTION=create" > build.env
          set +e
          pcluster create-cluster --cluster-configuration ./config/compute-cluster.yaml --cluster-name hpc-cluster --dryrun $DRYRUN 2>&1 > pcluster-output.log
          pcluster_status=$?
          set -e
      fi
    - |
      cat pcluster-output.log
      if [ "$DRYRUN" = "false" ]; then
          test $pcluster_status -eq 0
          pcluster describe-cluster --cluster-name hpc-cluster
      else
          cat pcluster-output.log | grep -q "Request would have succeeded, but DryRun flag is set\|No changes found"
      fi

# the update/create commands always return with an error code when you use --dryrun true, even when the validation succeeded and there are no errors => parsing the output afterwards
validate:
  stage: start
  variables:
    DRYRUN: "true"
  extends: .update_or_create_cluster


deploy:
  stage: deploy
  variables:
    DRYRUN: "false"
  extends: .update_or_create_cluster
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  artifacts:
    reports:
      dotenv: build.env


post_deploy:
  stage: deploy
  extends: .pcluster
  needs:
    - deploy
  script:
    - |
      if [ "$PCLUSTER_ACTION" = "create" ]; then
          bash scripts/setup_lustre_dra.sh
          bash scripts/update_dns_record.sh
          bash scripts/setup_lustre_logs.sh
      else # PCLUSTER_ACTION = update
          cluster_status=$(pcluster describe-cluster --cluster-name hpc-cluster --query clusterStatus)
          while [[ "$cluster_status" != *"UPDATE_COMPLETE"* ]]; do
              sleep 10
              echo "Waiting for cluster to finish updating  (current status: $fleet_status)..."
              cluster_status=$(pcluster describe-cluster --cluster-name hpc-cluster --query clusterStatus)
          done
          echo "Done. restarting compute fleet"
          pcluster update-compute-fleet --cluster-name hpc-cluster --status START_REQUESTED
      fi
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH


push_scripts:
  stage: deploy
  extends: .aws-cli
  script:
    - aws s3 cp ./scripts/create_users.py s3://sboinfrastructureassets/scripts/
    - aws s3 cp ./scripts/all_or_nothing_allocation.sh s3://sboinfrastructureassets/scripts/
    - aws s3 cp ./scripts/setup_lustre_directories.py s3://sboinfrastructureassets/scripts/
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      changes:
        - scripts/create_users.py
        - scripts/all_or_nothing_allocation.sh
        - scripts/setup_lustre_directories.py

destroy:
  stage: start
  extends: .pcluster
  when: manual
  allow_failure: true
  script:
    - |
      set +e
      pcluster delete-cluster --cluster-name hpc-cluster
      cluster_status=$(pcluster describe-cluster --cluster-name hpc-cluster --query clusterStatus)
      while [[ "${cluster_status}" == *"DELETE_IN_PROGRESS"* ]]; do
          echo "Waiting for the cluster to be deleted..."
          sleep 10
          cluster_status=$(pcluster describe-cluster --cluster-name hpc-cluster --query clusterStatus)
      done
      set -e
      echo ${cluster_status}

print_info:
  stage: info
  extends: .pcluster
  when: manual
  allow_failure: true
  script:
    - pcluster version
    - pcluster list-official-images
    - aws imagebuilder list-components --owner Self
    - pcluster list-images --image-status AVAILABLE
    - pcluster list-images --image-status PENDING
    - pcluster list-images --image-status FAILED
    - pcluster list-clusters
    - |
      if $(pcluster list-clusters | grep -q hpc-cluster) ; then
          pcluster describe-cluster --cluster-name hpc-cluster
          pcluster describe-compute-fleet --cluster-name hpc-cluster
      fi

print_instance_availabilities:
  stage: info
  extends: .aws-cli
  when: manual
  allow_failure: true
  script:
    - aws ec2 describe-reserved-instances-offerings --filters "Name=scope,Values=Availability Zone" --no-include-marketplace --instance-type t3.micro --region us-east-1
    - aws ec2 describe-reserved-instances-offerings --filters "Name=scope,Values=Availability Zone" --no-include-marketplace --instance-type c6i.metal --region us-east-1
    - aws ec2 describe-reserved-instances-offerings --filters "Name=scope,Values=Availability Zone" --no-include-marketplace --instance-type c6id.metal --region us-east-1

export_cluster_logs:
  stage: info
  extends: .pcluster
  when: manual
  allow_failure: true
  script:
    - pcluster export-cluster-logs --cluster-name hpc-cluster --bucket sboinfrastructureassets --bucket-prefix logs --output-file "log_archive_$(date +%Y%m%d_%H%M%S).tar.gz"
  artifacts:
    paths:
      - log_archive_*.tar.gz

update_components:
  stage: deploy
  extends: .aws-cli
  when: manual
  script:
    - echo "no-op" # update manually if components change - yes it's ugly
      #- aws s3 cp ./config/singularity_component.yaml s3://sboinfrastructureassets/components/
      #- aws imagebuilder create-component --name singularity --semantic-version "1.0.3" --change-description "Improve error handling" --platform "Linux" --uri "s3://sboinfrastructureassets/components/singularity_component.yaml"

build_image:
  stage: deploy
  extends: .pcluster
  when: manual
  script:
    - pcluster build-image --image-configuration ./config/custom_al2_ami_config.yaml --image-id alinux2-sbo-pcluster-v2
    - pcluster list-images --image-status PENDING

delete_image:
  stage: deploy
  extends: .pcluster
  when: manual
  variables:
    IMAGE_ID: ""
  script:
    - pcluster delete-image  --image-id $IMAGE_ID

build_image_status:
  stage: info
  extends: .pcluster
  when: manual
  script:
    - pcluster list-images --image-status PENDING

start_compute_fleet:
  stage: info
  extends: .pcluster
  when: manual
  script:
    - |
      pcluster update-compute-fleet --cluster-name hpc-cluster --status START_REQUESTED
      fleet_status=$(pcluster describe-compute-fleet --cluster-name hpc-cluster --query status)
      while [[ "$fleet_status" != *"RUNNING"* ]]; do
          sleep 5
          echo "Waiting for fleet to stop (current status: $fleet_status)..."
          fleet_status=$(pcluster describe-compute-fleet --cluster-name hpc-cluster --query status)
      done
      echo "Done."

stop_compute_fleet:
  stage: info
  extends: .pcluster
  when: manual
  script:
    - |
      pcluster update-compute-fleet --cluster-name hpc-cluster --status STOP_REQUESTED
      fleet_status=$(pcluster describe-compute-fleet --cluster-name hpc-cluster --query status)
      while [[ "$fleet_status" != *"STOPPED"* ]]; do
          sleep 5
          echo "Waiting for fleet to stop (current status: $fleet_status)..."
          fleet_status=$(pcluster describe-compute-fleet --cluster-name hpc-cluster --query status)
      done
      echo "Done."

delete_cloudformation_stack:
  stage: info
  extends: .aws-cli
  when: manual
  script:
    - aws cloudformation list-stacks
    - aws cloudformation delete-stack --stack-name alinux2-sbo-pcluster-v1
    - aws cloudformation list-stacks

debug_image:
  stage: info
  extends: .pcluster
  when: manual
  script:
    - pcluster describe-image --image-id alinux2-sbo-pcluster-v1
    - pcluster list-image-log-streams --image-id alinux2-sbo-pcluster-v1 --query 'logStreams[*].logStreamName'
    - pcluster get-image-log-events --image-id alinux2-sbo-pcluster-v1 --log-stream-name 3.6.0/1 | tee build_log.json
  artifacts:
    paths:
      - build_log.json
