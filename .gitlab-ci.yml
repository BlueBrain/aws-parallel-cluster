workflow:
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_PIPELINE_SOURCE == "web"

stages:
 - info
 - build
 - start
 - deploy

.aws-cli:
  image: public.ecr.aws/aws-cli/aws-cli:2.13.26
  before_script:
    - mkdir -p ~/.aws/
    - echo -e "[default]\naws_access_key_id = ${AWS_ACCESS_KEY}\naws_secret_access_key = ${AWS_SECRET_ACCESS_KEY}\n" > ~/.aws/credentials
    - echo -e "[default]\nregion = us-east-1\noutput = json\n" > ~/.aws/config

.pcluster:
  image: public.ecr.aws/parallelcluster/pcluster-api:3.8.0
  before_script:
    - |
      # Install the aws cli
      yum install -y unzip jq
      curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
      unzip -q awscliv2.zip
      ./aws/install
    - mkdir -p ~/.aws/
    - echo -e "[default]\naws_access_key_id = ${AWS_ACCESS_KEY}\naws_secret_access_key = ${AWS_SECRET_ACCESS_KEY}\n" > ~/.aws/credentials
    - echo -e "[default]\nregion = us-east-1\noutput = json\n" > ~/.aws/config

.update_or_create_cluster:
  extends: .pcluster
  script:
    - set -x
    - pcluster list-clusters
    - |
      if $(pcluster list-clusters | grep -q hpc-cluster); then
          echo "Cluster exists, will attempt to update."
          if [ "$DRYRUN" = "true" ]; then
              echo "Dry-running - attempting with a fake cluster first"
              set +e
              pcluster create-cluster --cluster-configuration ./config/compute-cluster.yaml --cluster-name fufu-cluster --dryrun $DRYRUN 2>&1 > fufu-pcluster-output.log
              cat fufu-pcluster-output.log | grep -q "Request would have succeeded, but DryRun flag is set\|No changes found"
              exitcode=$?
              if [ "${exitcode}" != 0 ]; then
                  echo "Fake cluster failed"
                  cat fufu-pcluster-output.log
                  exit ${exitcode}
              fi
              set -e
              echo "Fake cluster succeeded, now let's try with the real cluster"
          fi
          echo "PCLUSTER_ACTION=update" > build.env
          set +e
          pcluster update-cluster --cluster-configuration ./config/compute-cluster.yaml --cluster-name hpc-cluster --dryrun $DRYRUN 2>&1 > pcluster-output.log
          pcluster_status=$?
          set -e
      else
          echo "Creating new cluster"
          echo "PCLUSTER_ACTION=create" > build.env
          set +e
          pcluster create-cluster --cluster-configuration ./config/compute-cluster.yaml --cluster-name hpc-cluster --dryrun $DRYRUN 2>&1 > pcluster-output.log
          pcluster_status=$?
          set -e
      fi
    - |
      cat pcluster-output.log
      echo "====="
      if [ "$DRYRUN" = "false" ]; then
          test $pcluster_status -eq 0
          pcluster describe-cluster --cluster-name hpc-cluster
      else
          if [ -z $CI_MERGE_REQUEST_IID ]; then
            cat pcluster-output.log | grep -q "Request would have succeeded, but DryRun flag is set\|No changes found"
          else
            pip install requests furl
            python check_validation_output.py
          fi
      fi

# the update/create commands always return with an error code when you use --dryrun true, even when the validation succeeded and there are no errors => parsing the output afterwards
validate:
  stage: start
  variables:
    DRYRUN: "true"
  extends: .update_or_create_cluster


deploy:
  stage: deploy
  variables:
    DRYRUN: "false"
  extends: .update_or_create_cluster
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
  artifacts:
    reports:
      dotenv: build.env


post_deploy:
  stage: deploy
  extends: .pcluster
  needs:
    - deploy
  script:
    - |
      if [ "$PCLUSTER_ACTION" = "create" ]; then
          bash scripts/setup_lustre_dra.sh
          bash scripts/wait_for_deployment.sh  # Note: Script waits until cluster is deployed
          bash scripts/setup_lustre_logs.sh
          bash scripts/update_dns_record.sh
      else # PCLUSTER_ACTION = update
          cluster_status=$(pcluster describe-cluster --cluster-name hpc-cluster --query clusterStatus)
          while [[ "$cluster_status" != *"UPDATE_COMPLETE"* ]]; do
              sleep 10
              echo "Waiting for cluster to finish updating  (current status: $cluster_status)..."
              cluster_status=$(pcluster describe-cluster --cluster-name hpc-cluster --query clusterStatus)
          done
          echo "Done. restarting compute fleet"
          pcluster update-compute-fleet --cluster-name hpc-cluster --status START_REQUESTED
      fi
  rules:
    - if: $CI_COMMIT_BRANCH == "main"


push_scripts:
  stage: deploy
  extends: .aws-cli
  script:
    - aws s3 sync ./config/s3 s3://sboinfrastructureassets/config/
    - aws s3 sync ./scripts/s3 s3://sboinfrastructureassets/scripts/
  rules:
    - if: $CI_COMMIT_BRANCH == "main"

destroy:
  stage: start
  extends: .pcluster
  when: manual
  allow_failure: true
  script:
    - |
      set +e
      pcluster delete-cluster --cluster-name hpc-cluster
      cluster_status=$(pcluster describe-cluster --cluster-name hpc-cluster --query clusterStatus)
      while [[ "${cluster_status}" == *"DELETE_IN_PROGRESS"* ]]; do
          echo "Waiting for the cluster to be deleted..."
          sleep 10
          cluster_status=$(pcluster describe-cluster --cluster-name hpc-cluster --query clusterStatus)
      done
      set -e
      echo ${cluster_status}
  rules:
    - if: $CI_MERGE_REQUEST_SOURCE_BRANCH_NAME == "staging" && $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"

print_info:
  stage: info
  extends: .pcluster
  when: manual
  allow_failure: true
  script:
    - pcluster version
    - pcluster list-official-images
    - aws imagebuilder list-components --owner Self
    - pcluster list-images --image-status AVAILABLE
    - pcluster list-images --image-status PENDING
    - pcluster list-images --image-status FAILED
    - pcluster list-clusters
    - |
      if $(pcluster list-clusters | grep -q hpc-cluster) ; then
          pcluster describe-cluster --cluster-name hpc-cluster
          pcluster describe-compute-fleet --cluster-name hpc-cluster
      fi

print_instance_availabilities:
  stage: info
  extends: .aws-cli
  when: manual
  allow_failure: true
  script:
    - aws ec2 describe-reserved-instances-offerings --filters "Name=scope,Values=Availability Zone" --no-include-marketplace --instance-type t3.micro --region us-east-1
    - aws ec2 describe-reserved-instances-offerings --filters "Name=scope,Values=Availability Zone" --no-include-marketplace --instance-type c6i.metal --region us-east-1
    - aws ec2 describe-reserved-instances-offerings --filters "Name=scope,Values=Availability Zone" --no-include-marketplace --instance-type c6id.metal --region us-east-1

export_cluster_logs:
  stage: info
  extends: .pcluster
  when: manual
  allow_failure: true
  script:
    - pcluster export-cluster-logs --cluster-name hpc-cluster --bucket sboinfrastructureassets --bucket-prefix logs --output-file "log_archive_$(date +%Y%m%d_%H%M%S).tar.gz"
  artifacts:
    paths:
      - log_archive_*.tar.gz

update_components:
  stage: build
  extends: .aws-cli
  when: manual
  script:
    #- echo "no-op" # update manually if components change - yes it's ugly
    - aws s3 sync ./config/ami_components s3://sboinfrastructureassets/components
    - aws imagebuilder create-component --name singularity-ce --semantic-version "1.0.1" --change-description "Simplify test to avoid issues" --platform "Linux" --uri "s3://sboinfrastructureassets/components/singularity-ce.yaml"
      #- aws imagebuilder create-component --name awscli --semantic-version "1.0.0" --change-description "Initial commit AWS CLI component" --platform "Linux" --uri "s3://sboinfrastructureassets/components/awscli.yaml"
      #- aws imagebuilder create-component --name rpm-installer --semantic-version "1.0.0" --change-description "Initial commit RPM installer component" --platform "Linux" --uri "s3://sboinfrastructureassets/components/rpm_installer.yaml"

build_image:
  stage: build
  extends: .pcluster
  when: manual
  variables:
    IMAGE_ID: ""
  script:
    - pcluster build-image --image-configuration ./config/custom_al2_x86_ami_config.yaml --image-id ${IMAGE_ID}
    - pcluster list-images --image-status PENDING

delete_image:
  stage: build
  extends: .pcluster
  when: manual
  variables:
    IMAGE_ID: ""
  script:
    - pcluster delete-image --image-id $IMAGE_ID

build_image_status:
  stage: info
  extends: .pcluster
  when: manual
  script:
    - pcluster list-images --image-status PENDING

start_compute_fleet:
  stage: info
  extends: .pcluster
  when: manual
  script:
    - |
      pcluster update-compute-fleet --cluster-name hpc-cluster --status START_REQUESTED
      fleet_status=$(pcluster describe-compute-fleet --cluster-name hpc-cluster --query status)
      while [[ "$fleet_status" != *"RUNNING"* ]]; do
          sleep 5
          echo "Waiting for fleet to stop (current status: $fleet_status)..."
          fleet_status=$(pcluster describe-compute-fleet --cluster-name hpc-cluster --query status)
      done
      echo "Done."

stop_compute_fleet:
  stage: info
  extends: .pcluster
  when: manual
  script:
    - |
      pcluster update-compute-fleet --cluster-name hpc-cluster --status STOP_REQUESTED
      fleet_status=$(pcluster describe-compute-fleet --cluster-name hpc-cluster --query status)
      while [[ "$fleet_status" != *"STOPPED"* ]]; do
          sleep 5
          echo "Waiting for fleet to stop (current status: $fleet_status)..."
          fleet_status=$(pcluster describe-compute-fleet --cluster-name hpc-cluster --query status)
      done
      echo "Done."
  rules:
    - if: $CI_MERGE_REQUEST_SOURCE_BRANCH_NAME == "staging" && $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"

dra_describe:
  stage: info
  extends: .pcluster
  when: manual
  allow_failure: true
  script:
    - aws fsx describe-data-repository-associations

dra_redeploy:
  stage: info
  extends: .pcluster
  when: manual
  allow_failure: true
  script:
    - DRA_IDS=($(aws fsx describe-data-repository-associations --query Associations[*].AssociationId --output text))
    - for dra_id in ${DRA_IDS[@]}; do
    -   echo "Deleting DRA '${dra_id}' from Lustre FSx..."
    -   aws fsx delete-data-repository-association --association-id ${dra_id} --delete-data-in-file-system
    - done
    - while [[ -n $(aws fsx describe-data-repository-associations --query Associations[*].AssociationId --output text) ]]; do 
    -   echo "DRAs not deleted yet, waiting..."
    -   sleep 5s
    - done
    - bash ./scripts/setup_lustre_dra.sh

delete_cloudformation_stack:
  stage: info
  extends: .aws-cli
  when: manual
  script:
    - aws cloudformation list-stacks
    - aws cloudformation delete-stack --stack-name alinux2-sbo-pcluster-v1
    - aws cloudformation list-stacks
  rules:
    - if: $CI_MERGE_REQUEST_SOURCE_BRANCH_NAME == "staging" && $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"


debug_image:
  stage: build
  extends: .pcluster
  when: manual
  variables:
    IMAGE_ID: ""
  script:
    - pcluster describe-image --image-id ${IMAGE_ID}
    - pcluster list-image-log-streams --image-id ${IMAGE_ID} --query 'logStreams[*].logStreamName'
    - pcluster get-image-log-events --image-id ${IMAGE_ID} --log-stream-name 3.8.0/1 | tee build_log.json
  artifacts:
    paths:
      - build_log.json


run_cmd:
  stage: info
  extends: .pcluster
  when: manual
  variables:
    COMMAND: ""
  script:
    - echo -e "Running command:\n\t${COMMAND}"
    - ${COMMAND}
